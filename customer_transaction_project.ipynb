{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FemcvDOoYwWH"
   },
   "source": [
    "# Preface\n",
    "In todayâ€™s data-driven world, predictive analytics has become a cornerstone of decision-making across industries. In the banking sector, understanding customer behavior is crucial for enhancing services, minimizing risks, and identifying potential opportunities. This project focuses on predicting whether a customer will make a transaction in the future, irrespective of the transaction amount.\n",
    "\n",
    "The dataset provided comprises 202 columns, including anonymized features and a target variable indicating transaction behavior.Through this project, we aspire to gain deeper insights into customer behavior while developing a robust predictive model that can be applied to real-world scenarios in the banking domain. This preface sets the stage for the systematic exploration and problem-solving approach that follows in this report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wv11CWUeZRsl"
   },
   "source": [
    "# Problem Statement\n",
    "- Prepare a complete data analysis report on the given data.\n",
    "\n",
    "- Create a predictive model which will help the bank to identify which customer will make transactions in future.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HJhqafzoZgML"
   },
   "source": [
    "# Domain Analysis\n",
    "- Customer transaction prediction is used to predict whether the customer will make an transaction or not in feature. It is used in banking industry to identify potential customers.\n",
    "   - Dataset  of consist of 202 columns\n",
    "   - 1st column is ID_CODE, 2 nd is target column and remaining 200 columns are anonymized features with column name from var_1 to var_200\n",
    "\n",
    "__1. Id_code:__ Unique identifier for each row or record in the data.\n",
    "\n",
    "__2. Target:__ 0 means the customer will not do a transaction and 1 means the customer will do a transaction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "11jYYjstb90G"
   },
   "source": [
    "# Importing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YslFGZehcEWg"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV,RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from joblib import Parallel, parallel_backend\n",
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score,classification_report,confusion_matrix,fbeta_score,roc_curve,auc\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "277HkT4-dUBw"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns',None)\n",
    "pd.set_option('display.max_rows',None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8oHqTuUccPt"
   },
   "source": [
    "# Basic Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "du9nfR_PZnhx"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('customer.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "cwEmgdWMdGMH",
    "outputId": "4620c046-05ee-4e20-85ea-f756ed0c3d0e"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "DKMAy9KPcm04",
    "outputId": "d69d7113-067c-4b9e-d49d-0d9e4c153765"
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "id": "gIy3TngBcoEi",
    "outputId": "8f0704bb-6022-4181-8505-b6d5264302e4"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yogl1LyDcjR3",
    "outputId": "046129ec-a949-40dc-a4c2-74c9fd6516a1"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfLUVF6VdJBi"
   },
   "source": [
    "It consists of 200000 rows and 202 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "GlNeS2eCclLW",
    "outputId": "c60c958a-bbff-4b7e-b40f-18040bbe12d2"
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sV5KRT-XdPih"
   },
   "source": [
    "- The dataset has no constant columns, as no feature has zero variance.\n",
    "- Features are on different scales, indicating that normalization or standardization might be required before modeling.\n",
    "- In some features there high difference between 75th percentile and Max value, we need to handle outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPCnUdxzcqSm",
    "outputId": "89fb2065-74ae-4a28-b860-a68bbf16c9eb"
   },
   "outputs": [],
   "source": [
    "df.info(verbose=True,show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_W0mC8g0dyZ3"
   },
   "source": [
    "- It has 202 columns\n",
    "- There are 1 object column, 1 integer column and 200 float columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "R8TRXRgVcr2S",
    "outputId": "6046f82f-04c2-439e-ff5f-1ed374321e1d"
   },
   "outputs": [],
   "source": [
    "df[df.duplicated()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bvgUFXeKeY3G"
   },
   "source": [
    "There is no duplicate column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "n8WPoK5Hc5m5",
    "outputId": "16791a83-ee75-42b8-b11a-04fa7f43c84b"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jYkgyaVOfKsd"
   },
   "source": [
    "There is no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4J4Tgxnbkfyt"
   },
   "source": [
    "# Exploratory Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XzUXScLYkoBB"
   },
   "source": [
    "- We could not perform complete data analysis because as feature names are not provided.\n",
    "- we can plot the distibution of the feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aSg9EVPhlGNa"
   },
   "source": [
    "CHECKING DISTRIBUTION OF FIRST 100 FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UjfmYb3tlqbL"
   },
   "outputs": [],
   "source": [
    "df_100=df.iloc[:,2:102]\n",
    "count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 639
    },
    "id": "rTHqvEVdkj_y",
    "outputId": "71caddfb-5ce3-4c4a-a88f-ea60b5d47a6d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 30))\n",
    "count = 1\n",
    "for column in df_100.columns:\n",
    "    if count > 100:\n",
    "        break\n",
    "    plt.subplot(20, 5, count)\n",
    "    sns.axisgrid\n",
    "    sns.histplot(df_100[column])\n",
    "    count += 1\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TIPeTNjZoxzL"
   },
   "source": [
    "CHECKING DISTRIBUTION OF NEXT 100 FEATURE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d98opptBouAs"
   },
   "outputs": [],
   "source": [
    "df_200=df.iloc[:,103:202]\n",
    "count=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "3d-5uG3eo99j",
    "outputId": "262154e3-dcd6-409b-f966-a5c1ae2fd83f"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 30))\n",
    "count = 1\n",
    "for column in df_200.columns:\n",
    "    if count > 100:\n",
    "        break\n",
    "    ax=plt.subplot(20, 5, count)\n",
    "    sns.histplot(df_200[column],kde=True,ax=ax)\n",
    "    count += 1\n",
    "    sns.set_style('darkgrid')\n",
    "    ax.set_title(f'Histogram of {column}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YD60f3nooXLW"
   },
   "source": [
    "Most of the features follow normal distribution or close to normal distribution we don't need to perform transformation technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U0DAOPpdtYJ9"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WqDxGmEctfD4"
   },
   "source": [
    "## 1. Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 458
    },
    "id": "KI31L3IqteYa",
    "outputId": "41e79696-9a96-4424-eb11-04e619ad0c5f"
   },
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yOPydgoktpc4"
   },
   "source": [
    "There is no values in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_UGv_9ipt4Nq"
   },
   "source": [
    "## 2. Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "aCf0VKGmxEsp",
    "outputId": "559bf9d9-60a5-4a70-f86d-813e4965e28e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 60))\n",
    "for i, column in enumerate(df.select_dtypes(include=[np.number]).columns):\n",
    "    if i >=200:\n",
    "        break\n",
    "    plt.subplot(40, 5, i+1)  # Adjust subplot size for many columns\n",
    "    sns.boxplot(df[column])\n",
    "    plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZbboNNJ2DWg"
   },
   "source": [
    "From boxplot we found that most of the features have outliers, so we need to impute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DXmsNNE82dwm"
   },
   "outputs": [],
   "source": [
    "data2=df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N04gxD6nKsxk"
   },
   "source": [
    "### winsorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "d0SUxylJLxRP",
    "outputId": "048c573a-908c-40de-aeaa-c5d41b3ed00a"
   },
   "outputs": [],
   "source": [
    "df['var_0'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUCsVD8ZKtok"
   },
   "outputs": [],
   "source": [
    "numerical_columns=df.drop(['target','ID_code'],axis=1).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z0UjYEr5K6Vt"
   },
   "outputs": [],
   "source": [
    "from scipy.stats.mstats import winsorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4EAqq64K7RS"
   },
   "outputs": [],
   "source": [
    "for i in numerical_columns:\n",
    "   data2[i]=winsorize(data2[i],limits=[0.01, 0.01])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "id": "uBW6VpKiNPLe",
    "outputId": "1c45d2bc-f3e2-48ec-dba8-07bc72008e4b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sQjhSGBoMUDM",
    "outputId": "070842a9-318b-4619-c011-bf5dbdc94d05",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25, 60))\n",
    "for i, column in enumerate(data2.select_dtypes(include=[np.number]).columns,start=1):\n",
    "  if i>200:\n",
    "    break\n",
    "  plt.subplot(40, 5, i)  # Adjust subplot size for many columns\n",
    "  sns.boxplot(data2[column])\n",
    "  plt.title(column)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers has been capped using winsorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in numerical_columns:\n",
    "   df[i]=winsorize(df[i],limits=[0.01, 0.01])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler \n",
    "stan_scaler=StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stan_scaled=stan_scaler.fit_transform(df.iloc[:,2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[:,2:]=stan_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df['target'].value_counts().plot(kind='pie', autopct='%1.1f%%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=df['target'].value_counts()\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x='target', data=df, palette='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "- we found that there is huge imbalance in target column\n",
    "- target 1 is 10% and target 0 is 90%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['ID_code'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df=df.iloc[:,1:]\n",
    "y_df=df.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=42)\n",
    "x_df,y_df=smote.fit_resample(x_df,y_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix_1=x_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(120,120))\n",
    "sns.heatmap(corr_matrix_1,annot=True,cmap='Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation:\n",
    "- we can't interpret the heatmap we create a loop to find the feature which has high correlation within the features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.90\n",
    "high_corr_pairs = []\n",
    "for i in range(len(corr_matrix_1.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix_1.iloc[i,j])>threshold:\n",
    "            feature_1=corr_matrix_1.columns[i]\n",
    "            feature_2=corr_matrix_1.columns[j]\n",
    "            corr_value=corr_matrix_1.iloc[i, j]\n",
    "            high_corr_pairs.append([feature_1, feature_2, corr_value])\n",
    "high_corr_df = pd.DataFrame(high_corr_pairs, columns=[\"Feature 1\", \"Feature 2\", \"Correlation\"])\n",
    "\n",
    "high_corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "- Their is no highly correlated feature in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal Component Analysis\n",
    "- Principal Component Analysis (PCA) is a dimensionality reduction technique used in machine learning and statistics to reduce the number of features in a dataset while preserving as much variability (information) as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca=PCA()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit_transform(x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "sns.set_style('darkgrid')\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_),c='black',marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "value=np.cumsum(pca.explained_variance_ratio_)>=0.9\n",
    "n_components=np.argmax(value)+1\n",
    "n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observation\n",
    "- We use PCA n_compoenents =178 because it captures 90% variance of the dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_178=PCA(n_components=178)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df=pca_178.fit_transform(x_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df=pd.DataFrame(x_df,columns=['pca{}'.format(i)  for i in range(1,179)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_df,x_test_df,y_train_df,y_test_df=train_test_split(x_df,y_df,test_size=0.20,random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Displaying the algorithm results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance_display(model, X_train, y_train, X_test, y_test):\n",
    "  \n",
    "\n",
    "    # --- Training Data Evaluation ---\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    \n",
    "    # ROC Curve for Training Data\n",
    "    if hasattr(model, \"predict_proba\"):  # For models that have predict_proba() method\n",
    "        y_train_probs = model.predict_proba(X_train)[:, 1]\n",
    "    else:\n",
    "        y_train_probs = model.decision_function(X_train)  # For models like SVM\n",
    "    \n",
    "    fpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_probs)\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "    \n",
    "    print(f\"Model: {model.__class__.__name__}\")\n",
    "    print(\"\\nTraining Data Evaluation:\")\n",
    "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
    "    \n",
    "    # Plot ROC Curve for Training Data\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr_train, tpr_train, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_train:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve (Training Data)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()  # Adjust layout to fit without scrolling\n",
    "    plt.show()\n",
    "\n",
    "    # --- Test Data Evaluation ---\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    test_recall = classification_report(y_test, y_test_pred, output_dict=True)['1']['recall']\n",
    "    test_f2_score = fbeta_score(y_test, y_test_pred, beta=2, average='macro')\n",
    "    test_report = classification_report(y_test, y_test_pred)\n",
    "    test_conf_matrix = confusion_matrix(y_test, y_test_pred)\n",
    "    \n",
    "    print(\"\\nTest Data Evaluation:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Recall (Test Data): {test_recall:.4f}\")\n",
    "    print(f\"F2 Score: {test_f2_score:.4f}\")\n",
    "    print(\"Classification Report (Test Data):\\n\", test_report)\n",
    "    \n",
    "    # Plot Test Confusion Matrix\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    sns.heatmap(test_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "    plt.title(f\"Confusion Matrix - {model.__class__.__name__} (Test Data)\")\n",
    "    plt.xlabel(\"Predicted Label\")\n",
    "    plt.ylabel(\"True Label\")\n",
    "    plt.tight_layout()  # Adjust layout to fit without scrolling\n",
    "    plt.show()\n",
    "\n",
    "    # ROC Curve for Test Data\n",
    "    if hasattr(model, \"predict_proba\"):  # For models that have predict_proba() method\n",
    "        y_test_probs = model.predict_proba(X_test)[:, 1]\n",
    "    else:\n",
    "        y_test_probs = model.decision_function(X_test)  # For models like SVM\n",
    "    \n",
    "    fpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_probs)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "    \n",
    "    # Plot ROC Curve for Test Data\n",
    "    plt.figure(figsize=(6, 4))\n",
    "    plt.plot(fpr_test, tpr_test, color='blue', lw=2, label=f'ROC curve (AUC = {roc_auc_test:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='gray', linestyle='--', label='Random guess')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (Recall)')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve (Test Data)')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.tight_layout()  # Adjust layout to fit without scrolling\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Creating the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(model, x_train_df, y_train_df, x_test_df, y_test_df, show_plots=True):\n",
    "    # Predictions for training data\n",
    "    y_train_pred = model.predict(x_train_df)\n",
    "    train_accuracy = accuracy_score(y_train_df, y_train_pred)\n",
    "\n",
    "    # Probabilities or decision function for training data\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_train_probs = model.predict_proba(x_train_df)[:, 1]\n",
    "    else:\n",
    "        y_train_probs = model.decision_function(x_train_df)\n",
    "\n",
    "    # Compute ROC AUC for training data\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train_df, y_train_probs)\n",
    "    roc_auc_train = auc(fpr_train, tpr_train)\n",
    "\n",
    "    # Predictions for testing data\n",
    "    y_test_pred = model.predict(x_test_df)\n",
    "    test_accuracy = accuracy_score(y_test_df, y_test_pred)\n",
    "\n",
    "    # Probabilities or decision function for testing data\n",
    "    if hasattr(model, \"predict_proba\"):\n",
    "        y_test_probs = model.predict_proba(x_test_df)[:, 1]\n",
    "    else:\n",
    "        y_test_probs = model.decision_function(x_test_df)\n",
    "\n",
    "    # Compute ROC AUC for testing data\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test_df, y_test_probs)\n",
    "    roc_auc_test = auc(fpr_test, tpr_test)\n",
    "\n",
    "  \n",
    "    # Return results as a dictionary\n",
    "    return {\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Test Accuracy\": test_accuracy,\n",
    "        \"AUC (Train)\": roc_auc_train,\n",
    "        \"AUC (Test)\": roc_auc_test\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "logistic = LogisticRegression()  # or 'saga'\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "'penalty': ['l1', 'l2', 'elasticnet', None],  \n",
    "    'C': [0.01, 0.1, 1, 10, 100],              \n",
    "    'solver': ['liblinear', 'saga'],           \n",
    "    'max_iter': [100, 200, 500] \n",
    "}\n",
    "grid_log_df = GridSearchCV(estimator=logistic, param_grid=param_grid, scoring='accuracy', verbose=2, n_jobs=-1,cv=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with parallel_backend('multiprocessing'):\n",
    "  grid_log_df.fit(x_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_df = grid_log_df.best_estimator_\n",
    "best_model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results=pd.DataFrame(grid_log_df.cv_results_)\n",
    "results[results['rank_test_score']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_log_df.best_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_performance_display(best_model_df,x_train_df,y_train_df,x_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "Random=RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={\n",
    "    'n_estimators':[10,50,100],\n",
    "    'max_depth':[10, 20, 30],\n",
    "    'min_samples_split':[2, 5, 10], \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df=GridSearchCV(estimator=Random,param_grid=parameters,cv=3,verbose=2,n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend('multiprocessing'):\n",
    "  random_df.fit(x_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results=pd.DataFrame(random_df.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_df.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_par_forest=random_df.best_estimator_\n",
    "best_par_forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "evaluate_model_performance_display(best_par_forest,x_train_df,y_train_df,x_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from scipy.stats import uniform\n",
    "\n",
    "xgb_model = xgb.XGBClassifier(eval_metric='mlogloss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],              \n",
    "    'learning_rate': [0.01, 0.1],              \n",
    "    'max_depth': [5, 7],                       \n",
    "    'subsample': [0.7, 0.8],                 \n",
    "    'colsample_bytree': [0.7, 0.8],           \n",
    "    'gamma': [0, 0.1],                        \n",
    "    'min_child_weight': [3, 5]                 \n",
    "}\n",
    "\n",
    "grid_search = RandomizedSearchCV(estimator=xgb_model, \n",
    "                           param_distributions=param_grid, \n",
    "                           scoring='accuracy', \n",
    "                           cv=3, \n",
    "                           verbose=2, \n",
    "                           n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with parallel_backend('multiprocessing'):\n",
    "  grid_search.fit(x_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results=pd.DataFrame(grid_search.cv_results_)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_param=grid_search.best_estimator_\n",
    "best_param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance_display(best_param,x_train_df,y_train_df,x_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light BGM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "light_model=lgb.LGBMClassifier(verbose=-1)\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'learning_rate': [ 0.1, 0.2],\n",
    "    'max_depth': [ 5, 7],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'subsample': [ 0.7, 0.8],\n",
    "    'colsample_bytree': [0.7, 0.8]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg1=GridSearchCV(estimator=light_model,param_grid=param_grid,n_jobs=-1,scoring='accuracy',cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with parallel_backend('multiprocessing'):\n",
    "  lg1.fit(x_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res=pd.DataFrame(lg1.cv_results_)\n",
    "res[res['rank_test_score']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_param_1=lg1.best_estimator_\n",
    "best_param_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model_performance_display(best_param_1,x_train_df,y_train_df,x_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Function is used to print the model comparision dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_performance(models, x_train_df, y_train_df, x_test_df, y_test_df):\n",
    "    all_model_results = []\n",
    "\n",
    "\n",
    "    for model, model_name in models:\n",
    "       \n",
    "        model_results = evaluate_model_performance(model, x_train_df, y_train_df, x_test_df, y_test_df, show_plots=False)\n",
    "\n",
    "        train_accuracy = model_results[\"Train Accuracy\"]\n",
    "        test_accuracy = model_results[\"Test Accuracy\"]\n",
    "        train_auc = model_results[\"AUC (Train)\"]\n",
    "        test_auc = model_results[\"AUC (Test)\"]\n",
    "       \n",
    "    \n",
    "        all_model_results.append({\n",
    "            \"Model Name\": model_name,\n",
    "            \"Train Accuracy\": train_accuracy,\n",
    "            \"Test Accuracy\": test_accuracy,\n",
    "            \"Train AUC\": train_auc,\n",
    "            \"Test AUC\": test_auc\n",
    "        })\n",
    "        \n",
    "\n",
    "    model_performance_df = pd.DataFrame(all_model_results)\n",
    "    return model_performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "models = [\n",
    "    (best_model_df, \"Logistic Regression\"),\n",
    "    (best_par_forest, \"Random Forest\"),\n",
    "    (best_param, \"XGBoost\"),\n",
    "    (best_param_1, \"LightGBM\")\n",
    "]\n",
    "\n",
    "model_performance_dataframe = create_model_performance(models, x_train_df, y_train_df, x_test_df, y_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_performance_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Submission: Machine Learning Model Evaluation for Customer Transaction Prediction\n",
    "\n",
    "**Objective:** The goal of this project is to predict whether a customer will perform a transaction based on their features. \n",
    "\n",
    "**Models Evaluated:**\n",
    "1.   Logistic Regression (LR)\n",
    "2.   Random Forest Classifier (RFC)\n",
    "3.   XGBoost Classifier\n",
    "4.   Light Gradient Boosting Machine (LGBM)\n",
    "\n",
    "\n",
    "**Analysis:**\n",
    "*   Among all models, ***Light Gradient Boosting Machine (LGBM) and XGBoost Classifier achieved the highest accuracy***\n",
    "*   Both models demonstrate superior performance, outperforming others in handling the complexity of the dataset.\n",
    "*   Logistic Regression  had the lowest accuracies, indicating they are less suited for this problem.\n",
    "*   Random Forest has higherst accuarcy but the reason we didn't  choose as best model because of  high gap between training and testing accuracy\n",
    "\n",
    "**Conclusion:**\n",
    "\n",
    "Based on the accuracy metric:\n",
    "\n",
    "*   Light Gradient Boosting Machine (LGBM) and XGBoost Classifier are equally effective for the Customer Transaction Prediction\n",
    "*   Both models are recommended for deployment, considering their high accuracy and efficiency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Challenges:\n",
    "**Challenges Faced and Techniques Used**\n",
    "\n",
    "\n",
    "**1. Handling Outliers**\n",
    "\n",
    "**Challenge:**\n",
    "\n",
    "- Outliers in numerical features could distort the model and lead to overfitting or inaccurate predictions.\n",
    "\n",
    "**Solution**:\n",
    "\n",
    "- Winsorization was used to handle outliers.\n",
    "\n",
    "- The extreme values were capped at a specified percentile (e.g., 5th and 95th percentiles), ensuring that outliers did not disproportionately affect the modelâ€™s performance.\n",
    "\n",
    "**Reason for Technique**:\n",
    "\n",
    "- Winsorization preserves the structure of the data while controlling for the impact of extreme values. It is particularly useful when outliers are genuine data points but need to be contained for modeling.\n",
    "\n",
    "**2. High Dimensionality**\n",
    "\n",
    "**Challenge:**\n",
    "\n",
    "- The dataset contained a large number of features, leading to potential overfitting.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- Principal Component Analysis (PCA) was applied to reduce dimensionality.\n",
    "\n",
    "- PCA transformed the feature space into a lower-dimensional space, capturing the most variance while reducing redundant information.\n",
    "\n",
    "**Reason for Technique:**\n",
    "\n",
    "- PCA helps improve computational efficiency and reduces the risk of overfitting by retaining only the most important components.\n",
    "\n",
    "**3.Finding Optimal Hyperparameters**\n",
    "\n",
    "**Challenge:**\n",
    "\n",
    "- Determining the best hyperparameters for the machine learning model to maximize predictive accuracy.\n",
    "\n",
    "**Solution:**\n",
    "\n",
    "- Both Grid Search with Cross-Validation (Grid Search CV) and RandomizedSearchCV were implemented.\n",
    "\n",
    "- Grid Search CV exhaustively searched over specified parameter values while using cross-validation to evaluate performance.\n",
    "\n",
    "- RandomizedSearchCV sampled a fixed number of parameter settings from the specified distributions, allowing faster exploration of hyperparameters.\n",
    "\n",
    "**Reason for Technique:**\n",
    "\n",
    "- Grid Search CV ensures that the best combination of hyperparameters is selected by systematically exploring all possibilities and validating them on unseen folds of the data\n",
    "- RandomizedSearchCV complements this by providing a quicker search alternative, especially useful when the parameter space is large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  },
  "panel-cell-order": [
   "7580d18b-2549-4623-9345-75278d546fe4",
   "1a5f5558-b994-40a9-879a-4dc611cb2edc",
   "c43e321b-0741-43ec-acfc-5a7a27dd8ae6"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
